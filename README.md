# Recommendation Privacy : A Survey About Data Minimization and Unlearning in Recommendation Systems

In this repository, we collect papers for Recommendation Privacy, specificlly **Data minimization** and **Unlearning**
To see more details about each paper, click on the class title for a closer look.

- [GDPR](https://gdpr-info.eu/)
- [CCPA](https://oag.ca.gov/privacy/ccpa/regs)

## Data Minimization for Recommender System

<table>
  <tbody>
    <tr>
      <th>Title</th>
      <th align="center">Comments</th>
      <th align="center">Link</th>
    </tr>
    <!-- Paper list start here -->
    <tr>
      <td align="left">Operationalizing the Legal Principle of Data Minimization for Personalization</td>
      <td align="center">Basline</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2005.13718">
          WWW 2022
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">Dataset condensation for recommendation (Condensing Pre-augmented Recommendation Data via Lightweight Policy Gradient Estimation)</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2310.01038">
          TKDE 01.2025
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">Data-efficient Fine-tuning for LLM-based Recommendation</td>
      <td align="center">Influence function</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2401.17197">
        SIGIR 2024
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">Data Minimization for GDPR Compliance in  Machine Learning Models</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://link.springer.com/article/10.1007/s43681-021-00095-8">
        AI & Ethics 2021
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">The Data Minimization Principle in Machine Learning</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2405.19471">
        Arxiv 2024
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">The trade-off between data minimization and fairness in collaborative filtering</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2410.07182">
        Arxiv 2024
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3531146.3533148">
        FAccT 2022
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->
    <tr>
      <td align="left">“I’m not convinced that they don’t collect more than is necessary”:  User-Controlled Data Minimization Design in Search Engines</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://www.usenix.org/conference/usenixsecurity24/presentation/sharma">
        USENIX 2024
        </a>
      </td>
    </tr>
    <!-- This is on paper, follow this format -->


  </tbody>
</table>

## Unlearning for Recommender System

<table>
  <tbody>
    <tr>
      <th>Title</th>
      <th align="center">Comments</th>
      <th align="center">Link</th>
    </tr>
    <!-- Updated paper list with comments -->
    <tr>
      <td align="left">Machine Unlearning</td>
      <td align="center">Foundational approach</td>
      <td align="center">
        <a href="https://arxiv.org/abs/1912.03817">
          TCSP 2021
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Graph Unlearning</td>
      <td align="center">Unlearning in graph</td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3548606.3559352">
          CCS 2022
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Recommendation Unlearning</td>
      <td align="center">Unlearning in Rec </td>
      <td align="center">
        <a href="https://arxiv.org/abs/2201.06820">
          WWW 2022
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2308.07707">
          AAAI 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">What makes unlearning hard and what to do about it</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2406.01257">
          NIPS 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Influence</td>
      <td align="center">Benchmark</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2408.14393">
          NIPS 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Scalability of memorization-based machine unlearning</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://arxiv.org/abs/2410.16516">
          NIPS 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Post-Training Attribute Unlearning in Recommender Systems</td>
      <td align="center"></td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3701987">
          TOIS 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">Recommendation Unlearning via Influence Function</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3701763">
          TOIS 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">On the Effectiveness of Unlearning in Session-Based Recommendation</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3616855.3635823">
          WSDM 2024
        </a>
      </td>
    </tr>
    <tr>
      <td align="left">A General Strategy Graph Collaborative Filtering for Recommendation Unlearning</td>
      <td align="center">-</td>
      <td align="center">
        <a href="https://dl.acm.org/doi/10.1145/3627673.3679637">
          CIKM 2024
        </a>
      </td>
    </tr>
    <!-- End of updated paper list -->
  </tbody>
</table>


<!-- 
## [Data Minimization for Machine Learning](/DM4ML.md)

### Survey
- [A Comprehensive Survey of Dataset Distillation](https://arxiv.org/abs/2301.05603)
- [Data Distillation: A Survey](https://arxiv.org/abs/2301.04272)

### General paper
- [Dataset Regeneration for Sequential Recommendation](https://arxiv.org/abs/2405.17795)
- [Large-scale Dataset Pruning with Dynamic Uncertainty](https://arxiv.org/abs/2306.05175)
- [Entropy Law The Story Behind Data Compression and LLM Performance](https://arxiv.org/abs/2407.06645)
- [RecRanker Instruction Tuning Large Language Model as Ranker for Topk Recommendation](https://arxiv.org/abs/2306.01495)
- [The Data Minimization Principle in Machine Learning](https://arxiv.org/abs/2405.19471v1)
- [Deep Learning on a Data Diet Finding Important Examples Early in Training](https://arxiv.org/abs/2103.12961)
- [Less Is Better Unweighted Data Subsampling via Influence Function](https://arxiv.org/abs/2110.14034)
- [InfoBatch Lossless Training Speed Up by Unbiased Dynamic Data Pruning](https://arxiv.org/abs/2303.04947)
- [Coverage-centric Coreset Selection for High Pruning Rates](https://arxiv.org/abs/2210.15809)
- [Selection via Proxy Efficient Data Selection for Deep Learning](https://arxiv.org/abs/2306.00184)
- [Dataset Condensation via Efficient SyntheticData Parameterization](https://arxiv.org/abs/2206.00719)
- [Dataset Condensation with Gradient Matching](https://arxiv.org/abs/2202.07122)

## [LLM for Recommendation](/LLM4Rec.md)

### Survey
- [A Survey on Large Language Models for Recommendation](https://arxiv.org/abs/2305.19860)
- [How Can Recommender Systems Benefit from Large Language Models A Survey](https://arxiv.org/abs/2311.12338)

### General Paper
- [Recommender Systems in the Era of Large Language Models LLMs](https://arxiv.org/abs/2307.02046)
- [A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://arxiv.org/abs/2308.08434)
- [Recommendation as Instruction Following A Large Language Model Empowered Recommendation Approach](https://arxiv.org/abs/2306.01495)
- [Uncovering ChatGPTs Capabilities in Recommender Systems](https://arxiv.org/abs/2305.02182)

## [Privacy](/privacy.md)
- [No Free Lunch in Privacy for Free How does Dataset Condensation Help Privacy](https://arxiv.org/abs/2209.14987)
- [The Utility and Privacy Effects of a Click](https://arxiv.org/abs/2206.00240)
- [Privacy for Free How does Dataset Condensation Help Privacy](https://dl.acm.org/doi/abs/10.1145/3077136.3080783)

## [Other useful paper](/others.md)

- [LoRA LowRank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Recommender Systems with Generative Retrieval](https://arxiv.org/abs/2305.05065)
- [Our Model Achieves Excellent Performance on MovieLens What Does It Mean](https://arxiv.org/abs/2307.09985)
- [TALLRec An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation](https://arxiv.org/abs/2305.00447) -->
