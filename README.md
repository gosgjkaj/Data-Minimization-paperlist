# Paper list for Data Minimization (Recommender System)

This is a repository of papers collected specifically for Data Minimization in Recommender Systems. All contents are categorized into five classes as listed below. To see more details about each paper, click on the class title for a closer look.


## [Data Minimization for Recommender System](/DM4Rec.md)
- [Operationalizing the Legal Principle of Data Minimization for Personalization](https://arxiv.org/abs/2005.13718)
- [Dataset condensation for recommendation](https://arxiv.org/abs/2310.01038)
- [Leveraging Large Language Models LLMs to Empower TrainingFree Dataset Condensation for ContentBased Recommendation](https://arxiv.org/abs/2310.09874)
- [Data-efficient Fine-tuning for LLM-based Recommendation](https://arxiv.org/abs/2401.17197)

## [Data Minimization for Machine Learning](/DM4ML.md)

### Survey
- [A Comprehensive Survey of Dataset Distillation](https://arxiv.org/abs/2301.05603)
- [Data Distillation: A Survey](https://arxiv.org/abs/2301.04272)

### General paper
- [Dataset Regeneration for Sequential Recommendation](https://arxiv.org/abs/2405.17795)
- [Large-scale Dataset Pruning with Dynamic Uncertainty](https://arxiv.org/abs/2306.05175)
- [Entropy Law The Story Behind Data Compression and LLM Performance](https://arxiv.org/abs/2407.06645)
- [RecRanker Instruction Tuning Large Language Model as Ranker for Topk Recommendation](https://arxiv.org/abs/2306.01495)
- [The Data Minimization Principle in Machine Learning](https://arxiv.org/abs/2405.19471v1)
- [Deep Learning on a Data Diet Finding Important Examples Early in Training](https://arxiv.org/abs/2103.12961)
- [Less Is Better Unweighted Data Subsampling via Influence Function](https://arxiv.org/abs/2110.14034)
- [InfoBatch Lossless Training Speed Up by Unbiased Dynamic Data Pruning](https://arxiv.org/abs/2303.04947)
- [Coverage-centric Coreset Selection for High Pruning Rates](https://arxiv.org/abs/2210.15809)
- [Selection via Proxy Efficient Data Selection for Deep Learning](https://arxiv.org/abs/2306.00184)
- [Dataset Condensation via Efficient SyntheticData Parameterization](https://arxiv.org/abs/2206.00719)
- [Dataset Condensation with Gradient Matching](https://arxiv.org/abs/2202.07122)

## [LLM for Recommendation](/LLM4Rec.md)

### Survey
- [A Survey on Large Language Models for Recommendation](https://arxiv.org/abs/2305.19860)
- [How Can Recommender Systems Benefit from Large Language Models A Survey](https://arxiv.org/abs/2311.12338)

### General Paper
- [Recommender Systems in the Era of Large Language Models LLMs](https://arxiv.org/abs/2307.02046)
- [A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://arxiv.org/abs/2308.08434)
- [Recommendation as Instruction Following A Large Language Model Empowered Recommendation Approach](https://arxiv.org/abs/2306.01495)
- [Uncovering ChatGPTs Capabilities in Recommender Systems](https://arxiv.org/abs/2305.02182)

## [Privacy](/privacy.md)
- [No Free Lunch in Privacy for Free How does Dataset Condensation Help Privacy](https://arxiv.org/abs/2209.14987)
- [The Utility and Privacy Effects of a Click](https://arxiv.org/abs/2206.00240)
- [Privacy for Free How does Dataset Condensation Help Privacy](https://dl.acm.org/doi/abs/10.1145/3077136.3080783)

## [Other useful paper](/others.md)

- [LoRA LowRank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Recommender Systems with Generative Retrieval](https://arxiv.org/abs/2305.05065)
- [Our Model Achieves Excellent Performance on MovieLens What Does It Mean](https://arxiv.org/abs/2307.09985)
- [TALLRec An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation](https://arxiv.org/abs/2305.00447)
