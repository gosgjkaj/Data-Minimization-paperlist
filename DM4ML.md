## Data Minimization for Machine Learning

### Survey
- [A Comprehensive Survey of Dataset Distillation](https://arxiv.org/abs/2301.05603)
- [Data Distillation: A Survey](https://arxiv.org/abs/2301.04272)

### General paper
- [Dataset Regeneration for Sequential Recommendation](https://arxiv.org/abs/2405.17795)
- [Large-scale Dataset Pruning with Dynamic Uncertainty](https://arxiv.org/abs/2306.05175)
- [Entropy Law The Story Behind Data Compression and LLM Performance](https://arxiv.org/abs/2407.06645)
- [RecRanker Instruction Tuning Large Language Model as Ranker for Topk Recommendation](https://arxiv.org/abs/2306.01495)
- [The Data Minimization Principle in Machine Learning](https://arxiv.org/abs/2405.19471v1)
- [Deep Learning on a Data Diet Finding Important Examples Early in Training](https://arxiv.org/abs/2103.12961)
- [Less Is Better Unweighted Data Subsampling via Influence Function](https://arxiv.org/abs/2110.14034)
- [InfoBatch Lossless Training Speed Up by Unbiased Dynamic Data Pruning](https://arxiv.org/abs/2303.04947)
- [Coverage-centric Coreset Selection for High Pruning Rates](https://arxiv.org/abs/2210.15809)
- [Selection via Proxy Efficient Data Selection for Deep Learning](https://arxiv.org/abs/2306.00184)
- [Dataset Condensation via Efficient SyntheticData Parameterization](https://arxiv.org/abs/2206.00719)
- [Dataset Condensation with Gradient Matching](https://arxiv.org/abs/2202.07122)
